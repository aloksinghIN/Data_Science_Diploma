{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the spark session object and providing the appliaction name\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"NYC_Park_assignment\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the input file \n",
    "df=spark.read.format('csv').option(\"header\", \"true\").option(\"inferSchema\", \"true\").\\\n",
    "load('/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons Number|Plate ID|Registration State|         Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|    5092469481| GZH7067|                NY|2016-07-10 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0143A|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0400P|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23 00:00:00|             5|             SUBN|        FORD|                 0|              0|         0233P|\n",
      "|    8478629828| 66623ME|                NY|2017-06-14 00:00:00|            47|             REFG|       MITSU|                14|             14|         1120A|\n",
      "|    7868300310| 37033JV|                NY|2016-11-21 00:00:00|            69|             DELV|       INTER|                13|             13|         0555P|\n",
      "|    5096917368| FZD8593|                NY|2017-06-13 00:00:00|             7|             SUBN|       ME/BE|                 0|              0|         0852P|\n",
      "|    1413609545|  X20DCM|                NJ|2016-08-03 00:00:00|            40|              SDN|       TOYOT|                71|             71|         0215A|\n",
      "|    4628525523|  326SF9|                MA|2016-12-21 00:00:00|            36|               UT|         BMW|                 0|              0|         0758A|\n",
      "|    4627113330| HCA5464|                NY|2016-11-21 00:00:00|            36|             SUBN|       DODGE|                 0|              0|         1005A|\n",
      "|    4006478550| VAD7274|                VA|2016-10-05 00:00:00|             5|               4D|         BMW|                 0|              0|         0845A|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking few records of the dataframe\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summons Number: long (nullable = true)\n",
      " |-- Plate ID: string (nullable = true)\n",
      " |-- Registration State: string (nullable = true)\n",
      " |-- Issue Date: timestamp (nullable = true)\n",
      " |-- Violation Code: integer (nullable = true)\n",
      " |-- Vehicle Body Type: string (nullable = true)\n",
      " |-- Vehicle Make: string (nullable = true)\n",
      " |-- Violation Precinct: integer (nullable = true)\n",
      " |-- Issuer Precinct: integer (nullable = true)\n",
      " |-- Violation Time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking the metadata of the dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register dataframe as temp table to use spark-sql\n",
    "df.registerTempTable(\"nyc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|    min(Issue Date)|    max(Issue Date)|\n",
      "+-------------------+-------------------+\n",
      "|1972-03-30 00:00:00|2069-11-19 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# getting the time range for which the data belongs\n",
    "spark.sql(\"select min(`Issue Date`), max(`Issue Date`) from nyc\").show()\n",
    "\n",
    "# so we see that parking tickets data exits not only for year 2017, but it exists for other years as well.\n",
    "# Assumption - But as per TA, We are considering all records belong to Fiscal_Year_2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------+------------------+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|summary|      Summons Number|Plate ID|Registration State|    Violation Code| Vehicle Body Type|      Vehicle Make|Violation Precinct|  Issuer Precinct|   Violation Time|\n",
      "+-------+--------------------+--------+------------------+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|  count|            10803028|10803028|          10803028|          10803028|          10803028|          10803028|          10803028|         10803028|         10803028|\n",
      "|   mean|6.8174470290656595E9|Infinity|              99.0|34.599430455979565|3.9258887134586864| 6519.974025974026| 45.01216260848347|46.82931211508477|909.2857142857143|\n",
      "| stddev| 2.320233962328229E9|     NaN|               0.0|19.359868716323483|0.5013415469252523|18091.257389147086|40.552560268435805|62.66703577269467|791.8453853409226|\n",
      "|    min|          1002884949|   #1MOM|                99|                 0|                00|             ,FREI|                 0|                0|            .240P|\n",
      "|    max|          8585600044|       ~|                WY|                99|               nan|               nan|               933|              997|              nan|\n",
      "+-------+--------------------+--------+------------------+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking mean, meadin  and other attributes\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------+------------------+------------------+------------------+-----------------+------------------+-----------------+-----------------+\n",
      "|summary|     Summons Number| Plate ID|Registration State|    Violation Code| Vehicle Body Type|     Vehicle Make|Violation Precinct|  Issuer Precinct|   Violation Time|\n",
      "+-------+-------------------+---------+------------------+------------------+------------------+-----------------+------------------+-----------------+-----------------+\n",
      "|  count|           10803028| 10803028|          10803028|          10803028|          10803028|         10803028|          10803028|         10803028|         10803028|\n",
      "|   mean|6.817447029065661E9| Infinity|              99.0|34.599430455979565|3.9258887134586864|6519.974025974026| 45.01216260848347|46.82931211508477|909.2857142857143|\n",
      "| stddev|2.320233962328229E9|      NaN|               0.0|19.359868716323483|0.5013415469252522|18091.25738914709|40.552560268435805|62.66703577269466|791.8453853409226|\n",
      "|    min|         1002884949|    #1MOM|                99|                 0|                00|            ,FREI|                 0|                0|            .240P|\n",
      "|    25%|         5092100990| 384284.0|              99.0|                20|               4.0|              1.0|                 7|                1|            507.0|\n",
      "|    50%|         8108501210|2028691.0|              99.0|                36|               4.0|              7.0|                33|               28|            733.0|\n",
      "|    75%|         8485853180|2336798.0|              99.0|                40|               4.0|           1115.0|                78|               78|           1100.0|\n",
      "|    max|         8585600044|        ~|                WY|                99|               nan|              nan|               933|              997|              nan|\n",
      "+-------+-------------------+---------+------------------+------------------+------------------+-----------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking Bins, Percentile and some other attributes to perform eda\n",
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons Number|Plate ID|Registration State|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|\n",
      "+--------------+--------+------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|             0|     728|                 0|             0|            42711|       73050|                 0|              0|            63|\n",
      "+--------------+--------+------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check amount of null or NaN columns in data\n",
    "# Null values are found in Plate ID, Vehicle Body Type, Vehicle Make and Violation Time\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "parking_without_issuedate = df.drop('Issue Date')\n",
    "parking_without_issuedate\\\n",
    "        .select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in parking_without_issuedate.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan is replaced with None in Plate ID, Vehicle Body Type, Vehicle Make and Violation Time\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df = df.withColumn(\"Vehicle Body Type\",\\\n",
    "                             when(col(\"Vehicle Body Type\") == \"nan\", None).otherwise(col(\"Vehicle Body Type\")))\\\n",
    "                             .withColumn(\"Vehicle Make\",\\\n",
    "                             when(col(\"Vehicle Make\") == \"nan\", None).otherwise(col(\"Vehicle Make\")))\\\n",
    "                             .withColumn(\"Violation Time\",\\\n",
    "                             when(col(\"Violation Time\") == \"nan\", None).otherwise(col(\"Violation Time\")))\\\n",
    "                             .withColumn(\"Plate ID\",\\\n",
    "                             when(col(\"Plate ID\") == \"nan\", None).otherwise(col(\"Plate ID\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to use spark-sql, register dataframe as temp view \n",
    "df.createOrReplaceTempView(\"nyc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the data\n",
    "###### 1.Find the total number of tickets for the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|total_tickets|\n",
      "+-------------+\n",
      "|     10803028|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assumption - But as per TA, We are considering all records belong to Fiscal_Year_2017.\n",
    "\n",
    "spark.sql(\"select count(*) as total_tickets from nyc\").show()\n",
    "\n",
    "# So total tickets for the year is 10803028"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Find out the number of unique states from where the cars that got parking tickets came. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|total_tickets|\n",
      "+-------------+\n",
      "|           SC|\n",
      "|           AZ|\n",
      "|           NS|\n",
      "|           LA|\n",
      "|           MN|\n",
      "|           NJ|\n",
      "|           MX|\n",
      "|           DC|\n",
      "|           OR|\n",
      "|           99|\n",
      "+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checkout distinct Registration State\n",
    "spark.sql(\"select distinct(`Registration State`) as total_tickets from nyc\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|Registration State|   freq|\n",
      "+------------------+-------+\n",
      "|                NY|8481061|\n",
      "|                NJ| 925965|\n",
      "|                PA| 285419|\n",
      "|                FL| 144556|\n",
      "|                CT| 141088|\n",
      "|                MA|  85547|\n",
      "|                IN|  80749|\n",
      "|                VA|  72626|\n",
      "|                MD|  61800|\n",
      "|                NC|  55806|\n",
      "+------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the frequecncy of the various state and finding most frequent state\n",
    "spark.sql(\"select `Registration State`, count(*) as freq from nyc group by `Registration State` order by freq desc\").show(10)\n",
    "\n",
    "# 'NY' comes as most frequent state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace 99 to NY as NY is most frequent\n",
    "from pyspark.sql.functions import *\n",
    "df_state=df.withColumn('Registration State', \\\n",
    "                       when(col('Registration State') == (\"99\"), \"NY\").otherwise(col(\"Registration State\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make another view which has the above imputed values\n",
    "df_state.createOrReplaceTempView('imputed_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|Unique_states|\n",
      "+-------------+\n",
      "|           66|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Finding unique states\n",
    "spark.sql(\"select count(*) as Unique_states from (select distinct(`Registration State`) \\\n",
    "          as total_tickets from imputed_table) temp \").show()\n",
    "# So number of unique states from where the cars that got parking tickets came is 66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation tasks\n",
    "\n",
    "#### 1. How often does each violation code occur? Display the frequency of the top five violation codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+\n",
      "|Violation Code|freq_violation_code|\n",
      "+--------------+-------------------+\n",
      "|            21|            1528588|\n",
      "|            36|            1400614|\n",
      "|            38|            1062304|\n",
      "|            14|             893498|\n",
      "|            20|             618593|\n",
      "+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select `Violation Code`, count(*) as freq_violation_code from \\\n",
    "imputed_table group by `Violation Code` order by freq_violation_code desc\").show(5)\n",
    "\n",
    "# top five Violation codes are - 21,36,38,14,20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. How often does each 'vehicle body type' get a parking ticket? \n",
    "####     How about the 'vehicle make'? (Hint: Find the top 5 for both.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|Vehicle Body Type|freq_veh_body_type|\n",
      "+-----------------+------------------+\n",
      "|             SUBN|           3719802|\n",
      "|             4DSD|           3082020|\n",
      "|              VAN|           1411970|\n",
      "|             DELV|            687330|\n",
      "|              SDN|            438191|\n",
      "+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 5 'vehicle body type' which get parking tickets\n",
    "spark.sql(\"select `Vehicle Body Type`, count(*) as freq_veh_body_type \\\n",
    "from imputed_table group by `Vehicle Body Type` order by freq_veh_body_type desc\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|Vehicle Make|freq_veh_make|\n",
      "+------------+-------------+\n",
      "|        FORD|      1280958|\n",
      "|       TOYOT|      1211451|\n",
      "|       HONDA|      1079238|\n",
      "|       NISSA|       918590|\n",
      "|       CHEVR|       714655|\n",
      "+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 5 'Vehicle Make' which get parking tickets\n",
    "spark.sql(\"select `Vehicle Make`, count(*) as freq_veh_make \\\n",
    "from imputed_table group by `Vehicle Make` order by freq_veh_make desc\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. A precinct is a police station that has a certain zone of the city under its command. Find the (5 highest) frequencies of tickets for each of the following:\n",
    "#### 3.1 'Violation Precinct' (This is the precinct of the zone where the violation occurred). Using this, can you draw any insights for parking violations in any specific areas of the city?\n",
    "#### 3.2 'Issuer Precinct' (This is the precinct that issued the ticket.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|Violation Precinct|freq_voilation_pre|\n",
      "+------------------+------------------+\n",
      "|                19|            535671|\n",
      "|                14|            352450|\n",
      "|                 1|            331810|\n",
      "|                18|            306920|\n",
      "|               114|            296514|\n",
      "+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select `Violation Precinct`, count(*) as freq_voilation_pre \\\n",
    "from imputed_table where `Violation Precinct` !=0 group by `Violation Precinct` order by freq_voilation_pre desc\").show(5)\n",
    "\n",
    "# Top 5 Violation Precinct are 19, 14, 1, 18, 114 \n",
    "# Violation precinct 19 is top among all percinct when it comes to violations of the parking laws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|Issuer Precinct|freq_issuer_pre|\n",
      "+---------------+---------------+\n",
      "|             19|         521513|\n",
      "|             14|         344977|\n",
      "|              1|         321170|\n",
      "|             18|         296553|\n",
      "|            114|         289950|\n",
      "+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select `Issuer Precinct`, count(*) as freq_issuer_pre \\\n",
    "from imputed_table where `Issuer Precinct` !=0 group by `Issuer Precinct` order by freq_issuer_pre desc\").show(5)\n",
    "\n",
    "## Top 5 Issuer Precinct are 19, 14, 1, 18, 114 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Find the violation code frequencies for three precincts that have issued the most number of tickets. \n",
    "#### Do these precinct zones have an exceptionally high frequency of certain violation codes? \n",
    "#### Are these codes common across precincts? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+-------------------+\n",
      "|Violation Precinct|Violation Code|freq_voilation_code|\n",
      "+------------------+--------------+-------------------+\n",
      "|                19|            46|              90530|\n",
      "|                 1|            14|              76375|\n",
      "|                14|            14|              75850|\n",
      "|                19|            38|              74926|\n",
      "|                19|            37|              73359|\n",
      "+------------------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 3 Precinct are same in Issuer Precinct and Violation Precinct \n",
    "# i.e 19, 14 and 1\n",
    "# Note: 0 is taken as erroneaous as mentioned in the question earlier, so 0 is not taken among the top 3\n",
    "\n",
    "#First we calculate violation precinct frequency in Violation Precinct\n",
    "\n",
    "spark.sql(\"select `Violation Precinct`, `Violation Code`,  count(*) as freq_voilation_code \\\n",
    "from imputed_table where `Violation Precinct` !=0 and `Violation Precinct` in (19,14,1) \\\n",
    "group by `Violation Precinct`,`Violation Code` order by freq_voilation_code desc\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+-------------------+\n",
      "|Issuer Precinct|Violation Code|freq_voilation_code|\n",
      "+---------------+--------------+-------------------+\n",
      "|             19|            46|              86390|\n",
      "|             14|            14|              73837|\n",
      "|              1|            14|              73522|\n",
      "|             19|            37|              72437|\n",
      "|             19|            38|              72344|\n",
      "+---------------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we also check violation precinct frequency in Issuer Precinct\n",
    "spark.sql(\"select `Issuer Precinct`, `Violation Code`,  count(*) as freq_voilation_code \\\n",
    "from imputed_table where `Issuer Precinct` !=0 and `Issuer Precinct` in (19,14,1) \\\n",
    "group by `Issuer Precinct`,`Violation Code` order by freq_voilation_code desc\").show(5)\n",
    "\n",
    "# we see top three Precinct has most violation code as 46 and 14. These are common in both type of precinct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Find out the properties of parking violations across different times of the day:\n",
    "#### 5.1 Find a way to deal with missing values, if any.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|Null_Violation_Time|\n",
      "+-------------------+\n",
      "|                 63|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#counting of null Violation Time\n",
    "spark.sql(\"select count(*) as Null_Violation_Time from imputed_table where `Violation Time` is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|Not_Null_Violation_Time|\n",
      "+-----------------------+\n",
      "|               10802965|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Count non null rows before deleting them\n",
    "spark.sql(\"select count(*) as Not_Null_Violation_Time from imputed_table where `Violation Time` is not null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10802965"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing the rows which are having null violation time\n",
    "df_parking=df_state.dropna(subset=(\"Violation Time\"))\n",
    "df_parking.count()\n",
    "\n",
    "## dataframe row count has reduced from 10803028 to 10802965\n",
    "# 63 rows have been deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parking.createOrReplaceTempView(\"cleansed_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 The Violation Time field is specified in a strange format. Find a way to make this a time attribute that you can use to divide into groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Violation Time|\n",
      "+--------------+\n",
      "|         0143A|\n",
      "|         0400P|\n",
      "|         0233P|\n",
      "|         1120A|\n",
      "|         0555P|\n",
      "|         0852P|\n",
      "|         0215A|\n",
      "|         0758A|\n",
      "|         1005A|\n",
      "|         0845A|\n",
      "+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking the current time format\n",
    "spark.sql(\"select `Violation Time` from cleansed_time \").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|Violation Time|converted_time|\n",
      "+--------------+--------------+\n",
      "|         0143A|         01:43|\n",
      "|         0400P|         16:00|\n",
      "|         0233P|         14:33|\n",
      "|         1120A|         11:20|\n",
      "|         0555P|         17:55|\n",
      "|         0852P|         20:52|\n",
      "|         0215A|         02:15|\n",
      "|         0758A|         07:58|\n",
      "|         1005A|         10:05|\n",
      "|         0845A|         08:45|\n",
      "+--------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#converting the time format\n",
    "#Assuming the time format is first 2 character as hour, next 2 char is Minutes , next 1 char is A or P \n",
    "# A stands for AM\n",
    "# P stands for PM\n",
    "# hour is between 1-12\n",
    "# minute is in 0-59\n",
    "# # all other records which are not compliant with above assumptions will be filterred out\n",
    "\n",
    "spark.sql(\"select `Violation Time`,from_unixtime(unix_timestamp(concat(`Violation Time`,'M'),'hhmmaa'),'HH:mm') \\\n",
    "as converted_time from cleansed_time \").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|10744190|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# After conversion we found that some Violation Time can not be converted into proper format because of invalid input format.\n",
    "# for example- 8510P - is invalid time so convertion of this type of data gives null values. we discard such value. \n",
    "# checking if we have enough records after discarding invalid values.\n",
    "\n",
    "spark.sql(\"select count(*) from (select `Violation Time`, \\\n",
    "from_unixtime(unix_timestamp(concat(`Violation Time`,'M'),'hhmmaa'),'HH:mm') \\\n",
    "as converted_time from cleansed_time) t where t.converted_time is not null \").show()\n",
    "\n",
    "#10.7 million records seems enough for analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Divide 24 hours into six equal discrete bins of time. Choose the intervals as you see fit. For each of these groups, find the three most commonly occurring violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|Violation Time|converted_time|\n",
      "+--------------+--------------+\n",
      "|         0143A|            01|\n",
      "|         0400P|            16|\n",
      "|         0233P|            14|\n",
      "|         1120A|            11|\n",
      "|         0555P|            17|\n",
      "|         0852P|            20|\n",
      "|         0215A|            02|\n",
      "|         0758A|            07|\n",
      "|         1005A|            10|\n",
      "|         0845A|            08|\n",
      "+--------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking only hours\n",
    "spark.sql(\"select `Violation Time`, \\\n",
    "from_unixtime(unix_timestamp(concat(`Violation Time`,'M'),'hhmmaa'),'HH') \\\n",
    "as converted_time from cleansed_time\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+\n",
      "|hour_bin|Violation Code|count_violation_code|\n",
      "+--------+--------------+--------------------+\n",
      "|       3|            21|             1182676|\n",
      "|       3|            36|              751422|\n",
      "|       4|            36|              588395|\n",
      "|       4|            38|              462756|\n",
      "|       3|            38|              346518|\n",
      "|       4|            37|              337074|\n",
      "|       3|            14|              274284|\n",
      "|       4|            14|              256302|\n",
      "|       4|            46|              229325|\n",
      "|       4|            20|              219182|\n",
      "|       3|            46|              213694|\n",
      "|       5|            38|              203232|\n",
      "|       4|            71|              201379|\n",
      "|       3|            71|              192307|\n",
      "|       3|            20|              175688|\n",
      "|       4|            21|              148007|\n",
      "|       5|            37|              145784|\n",
      "|       5|            14|              144748|\n",
      "|       4|             7|              144533|\n",
      "|       2|            14|              141275|\n",
      "|       3|            40|              132487|\n",
      "|       5|             7|              131768|\n",
      "|       4|            40|              131294|\n",
      "|       2|            21|              119466|\n",
      "|       2|            40|              112186|\n",
      "|       4|            70|              106604|\n",
      "|       3|             7|              105144|\n",
      "|       3|            70|              101341|\n",
      "|       3|            37|              101230|\n",
      "|       4|            19|              101114|\n",
      "|       3|            19|               92093|\n",
      "|       5|            46|               85550|\n",
      "|       2|            20|               84647|\n",
      "|       5|            20|               83348|\n",
      "|       3|            69|               75293|\n",
      "|       4|            69|               71275|\n",
      "|       4|            31|               69018|\n",
      "|       5|            71|               67037|\n",
      "|       6|             7|               65593|\n",
      "|       3|            16|               65407|\n",
      "|       4|            16|               63285|\n",
      "|       5|             5|               63233|\n",
      "|       3|            47|               56224|\n",
      "|       1|            21|               53600|\n",
      "|       5|            40|               48107|\n",
      "|       6|            38|               47029|\n",
      "|       6|            14|               44778|\n",
      "|       1|            40|               44737|\n",
      "|       6|            40|               44541|\n",
      "|       2|             7|               44060|\n",
      "+--------+--------------+--------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dividing the records into bins\n",
    "#Assumotion- we divided the time in six bins (1,2,3,4,5,6). Below is the time slot information\n",
    "# 0-4 ->1\n",
    "# 4-8 ->2\n",
    "# 8-12 ->3\n",
    "# 12-16 ->4\n",
    "# 16-20 ->5\n",
    "# 20-24 ->6\n",
    "\n",
    "spark.sql(\"select t2.hour_bin,t2.`Violation Code`, count(t2.`Violation Code`) as count_violation_code from \\\n",
    "          (select *,\\\n",
    "          case \\\n",
    "          when (converted_time >=0 and converted_time <4) then 1 \\\n",
    "          when (converted_time >=4 and converted_time <8) then 2 \\\n",
    "          when (converted_time >=8 and converted_time <12) then 3 \\\n",
    "          when (converted_time >=12 and converted_time <16) then 4 \\\n",
    "          when (converted_time >=16 and converted_time <20) then 5 \\\n",
    "          when (converted_time >=20 and converted_time <24) then 6 \\\n",
    "          else null \\\n",
    "          end as hour_bin \\\n",
    "          from \\\n",
    "          (select *, \\\n",
    "from_unixtime(unix_timestamp(concat(`Violation Time`,'M'),'hhmmaa'),'HH') \\\n",
    "as converted_time from cleansed_time ) t where t.converted_time is not null) t2 \\\n",
    "group by t2.hour_bin,t2.`Violation Code` order by count_violation_code desc\").show(50)\n",
    "\n",
    "# for each of the hour_bin, we can get the most frequent violations by looking into the below output\n",
    "# Alternate way- I will apply filter on the hour_bin for each bin and execute the above query six time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+\n",
      "|hour_bin|Violation Code|count_violation_code|\n",
      "+--------+--------------+--------------------+\n",
      "|       1|            21|               53600|\n",
      "|       1|            40|               44737|\n",
      "|       1|            78|               28716|\n",
      "+--------+--------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternate Answer - finding three most commonly occurring violations for hour_bin=1\n",
    "spark.sql(\"select t2.hour_bin,t2.`Violation Code`, count(t2.`Violation Code`) as count_violation_code from \\\n",
    "          (select *,\\\n",
    "          case \\\n",
    "          when (converted_time >=0 and converted_time <4) then 1 \\\n",
    "          when (converted_time >=4 and converted_time <8) then 2 \\\n",
    "          when (converted_time >=8 and converted_time <12) then 3 \\\n",
    "          when (converted_time >=12 and converted_time <16) then 4 \\\n",
    "          when (converted_time >=16 and converted_time <20) then 5 \\\n",
    "          when (converted_time >=20 and converted_time <24) then 6 \\\n",
    "          else null \\\n",
    "          end as hour_bin \\\n",
    "          from \\\n",
    "          (select *, \\\n",
    "from_unixtime(unix_timestamp(concat(`Violation Time`,'M'),'hhmmaa'),'HH') \\\n",
    "as converted_time from cleansed_time ) t where t.converted_time is not null) t2 \\\n",
    "where t2.hour_bin=1  group by t2.hour_bin,t2.`Violation Code` order by count_violation_code desc\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+\n",
      "|hour_bin|Violation Code|count_violation_code|\n",
      "+--------+--------------+--------------------+\n",
      "|       2|            14|              141275|\n",
      "|       2|            21|              119466|\n",
      "|       2|            40|              112186|\n",
      "+--------+--------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternate Answer - finding three most commonly occurring violations for hour_bin=2\n",
    "spark.sql(\"select t2.hour_bin,t2.`Violation Code`, count(t2.`Violation Code`) as count_violation_code from \\\n",
    "          (select *,\\\n",
    "          case \\\n",
    "          when (converted_time >=0 and converted_time <4) then 1 \\\n",
    "          when (converted_time >=4 and converted_time <8) then 2 \\\n",
    "          when (converted_time >=8 and converted_time <12) then 3 \\\n",
    "          when (converted_time >=12 and converted_time <16) then 4 \\\n",
    "          when (converted_time >=16 and converted_time <20) then 5 \\\n",
    "          when (converted_time >=20 and converted_time <24) then 6 \\\n",
    "          else null \\\n",
    "          end as hour_bin \\\n",
    "          from \\\n",
    "          (select *, \\\n",
    "from_unixtime(unix_timestamp(concat(`Violation Time`,'M'),'hhmmaa'),'HH') \\\n",
    "as converted_time from cleansed_time ) t where t.converted_time is not null) t2 \\\n",
    "where t2.hour_bin=2  group by t2.hour_bin,t2.`Violation Code` order by count_violation_code desc\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+\n",
      "|hour_bin|Violation Code|count_violation_code|\n",
      "+--------+--------------+--------------------+\n",
      "|       3|            21|             1182676|\n",
      "|       3|            36|              751422|\n",
      "|       3|            38|              346518|\n",
      "+--------+--------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternate Answer - finding three most commonly occurring violations for hour_bin=3\n",
    "spark.sql(\"select t2.hour_bin,t2.`Violation Code`, count(t2.`Violation Code`) as count_violation_code from \\\n",
    "          (select *,\\\n",
    "          case \\\n",
    "          when (converted_time >=0 and converted_time <4) then 1 \\\n",
    "          when (converted_time >=4 and converted_time <8) then 2 \\\n",
    "          when (converted_time >=8 and converted_time <12) then 3 \\\n",
    "          when (converted_time >=12 and converted_time <16) then 4 \\\n",
    "          when (converted_time >=16 and converted_time <20) then 5 \\\n",
    "          when (converted_time >=20 and converted_time <24) then 6 \\\n",
    "          else null \\\n",
    "          end as hour_bin \\\n",
    "          from \\\n",
    "          (select *, \\\n",
    "from_unixtime(unix_timestamp(concat(`Violation Time`,'M'),'hhmmaa'),'HH') \\\n",
    "as converted_time from cleansed_time ) t where t.converted_time is not null) t2 \\\n",
    "where t2.hour_bin=3  group by t2.hour_bin,t2.`Violation Code` order by count_violation_code desc\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+\n",
      "|hour_bin|Violation Code|count_violation_code|\n",
      "+--------+--------------+--------------------+\n",
      "|       4|            36|              588395|\n",
      "|       4|            38|              462756|\n",
      "|       4|            37|              337074|\n",
      "+--------+--------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternate Answer - finding three most commonly occurring violations for hour_bin=4\n",
    "spark.sql(\"select t2.hour_bin,t2.`Violation Code`, count(t2.`Violation Code`) as count_violation_code from \\\n",
    "          (select *,\\\n",
    "          case \\\n",
    "          when (converted_time >=0 and converted_time <4) then 1 \\\n",
    "          when (converted_time >=4 and converted_time <8) then 2 \\\n",
    "          when (converted_time >=8 and converted_time <12) then 3 \\\n",
    "          when (converted_time >=12 and converted_time <16) then 4 \\\n",
    "          when (converted_time >=16 and converted_time <20) then 5 \\\n",
    "          when (converted_time >=20 and converted_time <24) then 6 \\\n",
    "          else null \\\n",
    "          end as hour_bin \\\n",
    "          from \\\n",
    "          (select *, \\\n",
    "from_unixtime(unix_timestamp(concat(`Violation Time`,'M'),'hhmmaa'),'HH') \\\n",
    "as converted_time from cleansed_time ) t where t.converted_time is not null) t2 \\\n",
    "where t2.hour_bin=4  group by t2.hour_bin,t2.`Violation Code` order by count_violation_code desc\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+\n",
      "|hour_bin|Violation Code|count_violation_code|\n",
      "+--------+--------------+--------------------+\n",
      "|       5|            38|              203232|\n",
      "|       5|            37|              145784|\n",
      "|       5|            14|              144748|\n",
      "+--------+--------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternate Answer - finding three most commonly occurring violations for hour_bin=5\n",
    "spark.sql(\"select t2.hour_bin,t2.`Violation Code`, count(t2.`Violation Code`) as count_violation_code from \\\n",
    "          (select *,\\\n",
    "          case \\\n",
    "          when (converted_time >=0 and converted_time <4) then 1 \\\n",
    "          when (converted_time >=4 and converted_time <8) then 2 \\\n",
    "          when (converted_time >=8 and converted_time <12) then 3 \\\n",
    "          when (converted_time >=12 and converted_time <16) then 4 \\\n",
    "          when (converted_time >=16 and converted_time <20) then 5 \\\n",
    "          when (converted_time >=20 and converted_time <24) then 6 \\\n",
    "          else null \\\n",
    "          end as hour_bin \\\n",
    "          from \\\n",
    "          (select *, \\\n",
    "from_unixtime(unix_timestamp(concat(`Violation Time`,'M'),'hhmmaa'),'HH') \\\n",
    "as converted_time from cleansed_time ) t where t.converted_time is not null) t2 \\\n",
    "where t2.hour_bin=5  group by t2.hour_bin,t2.`Violation Code` order by count_violation_code desc\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+\n",
      "|hour_bin|Violation Code|count_violation_code|\n",
      "+--------+--------------+--------------------+\n",
      "|       6|             7|               65593|\n",
      "|       6|            38|               47029|\n",
      "|       6|            14|               44778|\n",
      "+--------+--------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternate Answer - finding three most commonly occurring violations for hour_bin=6\n",
    "spark.sql(\"select t2.hour_bin,t2.`Violation Code`, count(t2.`Violation Code`) as count_violation_code from \\\n",
    "          (select *,\\\n",
    "          case \\\n",
    "          when (converted_time >=0 and converted_time <4) then 1 \\\n",
    "          when (converted_time >=4 and converted_time <8) then 2 \\\n",
    "          when (converted_time >=8 and converted_time <12) then 3 \\\n",
    "          when (converted_time >=12 and converted_time <16) then 4 \\\n",
    "          when (converted_time >=16 and converted_time <20) then 5 \\\n",
    "          when (converted_time >=20 and converted_time <24) then 6 \\\n",
    "          else null \\\n",
    "          end as hour_bin \\\n",
    "          from \\\n",
    "          (select *, \\\n",
    "from_unixtime(unix_timestamp(concat(`Violation Time`,'M'),'hhmmaa'),'HH') \\\n",
    "as converted_time from cleansed_time ) t where t.converted_time is not null) t2 \\\n",
    "where t2.hour_bin=6  group by t2.hour_bin,t2.`Violation Code` order by count_violation_code desc\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Now, try another direction. For the three most commonly occurring violation codes, find the most common time of the day (in terms of the bins from the previous part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|Violation Code|count_violation_code|\n",
      "+--------------+--------------------+\n",
      "|            21|             1504663|\n",
      "|            36|             1400614|\n",
      "|            38|             1062287|\n",
      "|            14|              885469|\n",
      "|            20|              614453|\n",
      "+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#finding 3 most occuring violaiton codes\n",
    "spark.sql(\"select t2.`Violation Code`, count(t2.`Violation Code`) as count_violation_code from \\\n",
    "          (select *,\\\n",
    "          case \\\n",
    "          when (converted_time >=0 and converted_time <4) then 1 \\\n",
    "          when (converted_time >=4 and converted_time <8) then 2 \\\n",
    "          when (converted_time >=8 and converted_time <12) then 3 \\\n",
    "          when (converted_time >=12 and converted_time <16) then 4 \\\n",
    "          when (converted_time >=16 and converted_time <20) then 5 \\\n",
    "          when (converted_time >=20 and converted_time <24) then 6 \\\n",
    "          else null \\\n",
    "          end as hour_bin \\\n",
    "          from \\\n",
    "          (select *, \\\n",
    "from_unixtime(unix_timestamp(concat(`Violation Time`,'M'),'hhmmaa'),'HH') \\\n",
    "as converted_time from cleansed_time ) t where t.converted_time is not null) t2 \\\n",
    "group by t2.`Violation Code` order by count_violation_code desc\").show(5)\n",
    "\n",
    "# so 3 most occuring violation codes are 21,36,38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|hour_bin|count_hour_bin|\n",
      "+--------+--------------+\n",
      "|       3|       2280616|\n",
      "|       4|       1199158|\n",
      "|       5|        230641|\n",
      "|       2|        155705|\n",
      "|       1|         54052|\n",
      "|       6|         47392|\n",
      "+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#finding the most common time of the day when most violation is occuring\n",
    "\n",
    "spark.sql(\"select t2.`hour_bin`, count(t2.`hour_bin`) as count_hour_bin from \\\n",
    "          (select *,\\\n",
    "          case \\\n",
    "          when (converted_time >=0 and converted_time <4) then '1' \\\n",
    "          when (converted_time >=4 and converted_time <8) then '2' \\\n",
    "          when (converted_time >=8 and converted_time <12) then '3' \\\n",
    "          when (converted_time >=12 and converted_time <16) then '4' \\\n",
    "          when (converted_time >=16 and converted_time <20) then '5' \\\n",
    "          when (converted_time >=20 and converted_time <24) then '6' \\\n",
    "          else null \\\n",
    "          end as hour_bin \\\n",
    "          from \\\n",
    "          (select *, \\\n",
    "from_unixtime(unix_timestamp(concat(`Violation Time`,'M'),'hhmmaa'),'HH') \\\n",
    "as converted_time from cleansed_time ) t where t.converted_time is not null) t2 \\\n",
    "where t2.`Violation Code` in (21,36,38)  group by t2.`hour_bin` order by count_hour_bin desc\").show(6)\n",
    "# most common time is 3rd slot, that is morning 8-12 most violations are happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. First, divide the year into a certain number of seasons, and find the frequencies of tickets for each season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------------------------------------------------------+\n",
      "|Issue Date|from_unixtime(unix_timestamp(Issue Date, yyyy-MM-dd), yyyy-MM-dd HH:mm:ss)|\n",
      "+----------+--------------------------------------------------------------------------+\n",
      "+----------+--------------------------------------------------------------------------+\n",
      "\n",
      "+-------------------+--------------------------------------------------------------------------+\n",
      "|         Issue Date|from_unixtime(unix_timestamp(Issue Date, yyyy-MM-dd), yyyy-MM-dd HH:mm:ss)|\n",
      "+-------------------+--------------------------------------------------------------------------+\n",
      "|2016-07-10 00:00:00|                                                       2016-07-10 00:00:00|\n",
      "|2016-07-08 00:00:00|                                                       2016-07-08 00:00:00|\n",
      "|2016-08-23 00:00:00|                                                       2016-08-23 00:00:00|\n",
      "|2017-06-14 00:00:00|                                                       2017-06-14 00:00:00|\n",
      "|2016-11-21 00:00:00|                                                       2016-11-21 00:00:00|\n",
      "+-------------------+--------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Assumptios\n",
    "# there will be four seasons in a year\n",
    "#1. Spring - runs from March 1 to May 31;\n",
    "#2. Summer - runs from June 1 to August 31;\n",
    "#3. Autumn - runs from September 1 to November 30\n",
    "#4. Winter - runs from December 1 to February 28 (February 29 in a leap year).\n",
    "# we will negelect the time to calculate season. we will only consider day and month to calculate season\n",
    "# Issue data is in yyyy-MM-dd format\n",
    "\n",
    "spark.sql(\"select `Issue Date`, from_unixtime(unix_timestamp(`Issue Date`,'yyyy-MM-dd')) from imputed_table where `Issue Date` is null \").show(5)\n",
    "\n",
    "spark.sql(\"select `Issue Date`, from_unixtime(unix_timestamp(`Issue Date`,'yyyy-MM-dd')) from  imputed_table  \").show(5)\n",
    "\n",
    "# we can see there is no null dates and spark has inferred the `Issue Date` as date correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------+\n",
      "|season|ticket_count_per_season|\n",
      "+------+-----------------------+\n",
      "|Spring|                2880687|\n",
      "|Autumn|                2830802|\n",
      "|Summer|                2606208|\n",
      "|Winter|                2485331|\n",
      "+------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#finding ticket count for each season\n",
    "spark.sql(\" select t2.season,count(t2.season) as ticket_count_per_season from \\\n",
    "    (select *,\\\n",
    "    case \\\n",
    "    when (t1.month in (3,4,5)) then 'Spring' \\\n",
    "    when (t1.month in (6,7,8)) then 'Summer' \\\n",
    "    when (t1.month in (9,10,11)) then 'Autumn' \\\n",
    "    when (t1.month in (12,1,2)) then 'Winter' \\\n",
    "    else null \\\n",
    "    end as season \\\n",
    "    from \\\n",
    "(select *, month(from_unixtime(unix_timestamp(`Issue Date`,'yyyy-MM-dd'))) as month \\\n",
    "from  imputed_table) t1 ) t2 group by season order by ticket_count_per_season desc\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|month|\n",
      "+-----+\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verification step to use season count as count the tickets\n",
    "spark.sql(\"select month(from_unixtime(unix_timestamp(`Issue Date`,'yyyy-MM-dd'))) as month \\\n",
    "from  imputed_table  where `Issue Date` is null \").show(5)\n",
    "# we check that there is no null for month so we can count season (which is derived from month) as no of ticketes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Then, find the three most common violations for each of these seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|Violation Code|count_violation_code|\n",
      "+--------------+--------------------+\n",
      "|            21|              402807|\n",
      "|            36|              344834|\n",
      "|            38|              271192|\n",
      "+--------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#three most common violations for Spring\n",
    "spark.sql(\" select t2.`Violation Code`,count(t2.`Violation Code`) as count_violation_code from \\\n",
    "    (select *,\\\n",
    "    case \\\n",
    "    when (t1.month in (3,4,5)) then 'Spring' \\\n",
    "    when (t1.month in (6,7,8)) then 'Summer' \\\n",
    "    when (t1.month in (9,10,11)) then 'Autumn' \\\n",
    "    when (t1.month in (12,1,2)) then 'Winter' \\\n",
    "    else null \\\n",
    "    end as season \\\n",
    "    from \\\n",
    "(select *, month(from_unixtime(unix_timestamp(`Issue Date`,'yyyy-MM-dd'))) as month \\\n",
    "from  imputed_table) t1 ) t2 where season='Spring' group by `Violation Code` \\\n",
    "order by count_violation_code desc\").show(3)\n",
    "#Most common violation for spring is 21,36,38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|Violation Code|count_violation_code|\n",
      "+--------------+--------------------+\n",
      "|            36|              456046|\n",
      "|            21|              357479|\n",
      "|            38|              283828|\n",
      "+--------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#three most common violations for Autumn\n",
    "spark.sql(\" select t2.`Violation Code`,count(t2.`Violation Code`) as count_violation_code from \\\n",
    "    (select *,\\\n",
    "    case \\\n",
    "    when (t1.month in (3,4,5)) then 'Spring' \\\n",
    "    when (t1.month in (6,7,8)) then 'Summer' \\\n",
    "    when (t1.month in (9,10,11)) then 'Autumn' \\\n",
    "    when (t1.month in (12,1,2)) then 'Winter' \\\n",
    "    else null \\\n",
    "    end as season \\\n",
    "    from \\\n",
    "(select *, month(from_unixtime(unix_timestamp(`Issue Date`,'yyyy-MM-dd'))) as month \\\n",
    "from  imputed_table) t1 ) t2 where season='Autumn' group by `Violation Code` \\\n",
    "order by count_violation_code desc\").show(3)\n",
    "#Most common violation for Autumn is 36,21,38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|Violation Code|count_violation_code|\n",
      "+--------------+--------------------+\n",
      "|            21|              405961|\n",
      "|            38|              247561|\n",
      "|            36|              240396|\n",
      "+--------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#three most common violations for Summer\n",
    "spark.sql(\" select t2.`Violation Code`,count(t2.`Violation Code`) as count_violation_code from \\\n",
    "    (select *,\\\n",
    "    case \\\n",
    "    when (t1.month in (3,4,5)) then 'Spring' \\\n",
    "    when (t1.month in (6,7,8)) then 'Summer' \\\n",
    "    when (t1.month in (9,10,11)) then 'Autumn' \\\n",
    "    when (t1.month in (12,1,2)) then 'Winter' \\\n",
    "    else null \\\n",
    "    end as season \\\n",
    "    from \\\n",
    "(select *, month(from_unixtime(unix_timestamp(`Issue Date`,'yyyy-MM-dd'))) as month \\\n",
    "from  imputed_table) t1 ) t2 where season='Summer' group by `Violation Code` \\\n",
    "order by count_violation_code desc\").show(3)\n",
    "#Most common violation for Summer is 21,38,36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|Violation Code|count_violation_code|\n",
      "+--------------+--------------------+\n",
      "|            21|              362341|\n",
      "|            36|              359338|\n",
      "|            38|              259723|\n",
      "+--------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#three most common violations for Winter\n",
    "spark.sql(\" select t2.`Violation Code`,count(t2.`Violation Code`) as count_violation_code from \\\n",
    "    (select *,\\\n",
    "    case \\\n",
    "    when (t1.month in (3,4,5)) then 'Spring' \\\n",
    "    when (t1.month in (6,7,8)) then 'Summer' \\\n",
    "    when (t1.month in (9,10,11)) then 'Autumn' \\\n",
    "    when (t1.month in (12,1,2)) then 'Winter' \\\n",
    "    else null \\\n",
    "    end as season \\\n",
    "    from \\\n",
    "(select *, month(from_unixtime(unix_timestamp(`Issue Date`,'yyyy-MM-dd'))) as month \\\n",
    "from  imputed_table) t1 ) t2 where season='Winter' group by `Violation Code` \\\n",
    "order by count_violation_code desc\").show(3)\n",
    "#Most common violation for spring is 21,36,38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. The fines collected from all the instances of parking violation constitute a source of revenue for the NYC Police Department. Lets take an example of estimating this for the three most commonly occurring codes:\n",
    "   #### 7.1 Find the total occurrences of the three most common violation codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|Violation Code|count_violation_code|\n",
      "+--------------+--------------------+\n",
      "|            21|             1528588|\n",
      "|            36|             1400614|\n",
      "|            38|             1062304|\n",
      "|            14|              893498|\n",
      "|            20|              618593|\n",
      "+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# finding three most common vioaltions\n",
    "\n",
    "spark.sql(\"select `Violation Code`,count(`Violation Code`) as count_violation_code \\\n",
    "from  imputed_table  group by `Violation Code` \\\n",
    "order by count_violation_code desc\").show(5)\n",
    "\n",
    "# so three most common violation codes are - 21, 36, 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|Violation Code| Revenue|\n",
      "+--------------+--------+\n",
      "|            21|84072340|\n",
      "|            36|70030700|\n",
      "|            38|53115200|\n",
      "+--------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As per the question info - Charges given for top 3 violation codes are as follows\n",
    "#code 21 - charges((65+45)/2=55)\n",
    "#code 36 - charges((50+50)/2=50)\n",
    "#code 38 - charges((65+35)/2=50)\n",
    "\n",
    "spark.sql(\"select t.`Violation Code`,t.count_violation_code*t.charges as Revenue from \\\n",
    "( select `Violation Code`,count(`Violation Code`) as count_violation_code, \\\n",
    "case \\\n",
    "when (`Violation Code`=21) then 55 \\\n",
    "when (`Violation Code`=36) then 50 \\\n",
    "when (`Violation Code`=38) then 50 \\\n",
    "else null \\\n",
    "end as charges \\\n",
    "from  imputed_table  group by `Violation Code` \\\n",
    "order by count_violation_code desc) t\").show(3)\n",
    "\n",
    "# So we see that Violaton code 21 has highest total collection which is 84072340 dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stooping the spark session to release the resources\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
